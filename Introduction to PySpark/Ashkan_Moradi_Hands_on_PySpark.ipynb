{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bf8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PySpark DataFrames is like Pandas DataFrames.\n",
    "The key difference in Spark is how the data is distributed. \n",
    "Pandas operates on a single compute instance,\n",
    "while PySpark distributes data across multiple instances.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f0b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Pyspark setup:\n",
    "\n",
    "*** 1) Intall pyspark by using \"pip install pyspark\" on terminal.\n",
    "*** 2) Install \"Adoptium Java 17\" which is compatible with PySpark versions 3.4 and more.\n",
    "*** 3) Set Java_Home system variable using the \"Adoptium Java 17\" path\n",
    "*** 4) Run this Pyspark Test code to check whether working on not.\n",
    "\"\"\" \n",
    "\n",
    "# Test\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"QuickTest\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "df = spark.range(5)\n",
    "df.show()\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "# If run without any error, means Pyspark is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a79668e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000028E5D999400>\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession from builder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession (set up or create a session)\n",
    "my_spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()\n",
    "print(my_spark)\n",
    "\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating DataFrames from various data sources:\n",
    "\n",
    "spark.read.csv(\"path/file_name.csv\")            => Read csv file (Comma Seperated Value)\n",
    "spark.read.json(\"path/file_name.json\")          => Read json file (JavaScript Object Notation)\n",
    "spark.read.parquet(\"path/file_name.parquet\")    => Read parquet file\n",
    "spark.read.schema()                             => Read schemas from database\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac40072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame using cvs file:\n",
    "\n",
    "# DataFrame_name = spark.read.csv('path', header=True, InferSchema=True)\n",
    "census_df = spark.read.csv(\"census.csv\", \n",
    "                [\"gender\",\"age\",\"zipcode\",\"salary_range_usd\",\"marriage_status\"])\n",
    "# Show the first 5 rows = of the DataFrame\n",
    "census_df.show()\n",
    "# Create DataFrame\n",
    "DataFrame_Name = spark.createDataFrame()\n",
    "# Inspect and show the schema (Columns and types)\n",
    "DataFrame_Name.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c86c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Opertaions with PySpark DataFrames:\n",
    "\n",
    ".select()                                       => to select\n",
    ".filter()                                       => to filter rows based on a condition\n",
    ".where()                                        => to filter match a specific value\n",
    ".groupBy().Aggregation_Type()                   => to group, summarize and aggregate on one column (Aggregation_Type: sum(), min() , max(), avg(),... *** All are lowercase)\n",
    ".groupBy().Agg()                                => to group and aggregate on more than one column using agg()\n",
    ".sort()                                         => to sort on one column\n",
    ".orderBy()                                      => to sort on more than one column\n",
    "df1.join(df2, on=\"column\", how=\"join_type\")     => to join two DataFrames with same name. join_type must be one of inner, left ,right, outer, right outer, left outer.\n",
    "df1.join(df2, column1=column2, \"join_type\")     => to join two DataFrames with different names \n",
    "df1.union(df2)                                  => to combine and append rows from two DataFrames (**with same schemas**) \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Some examples:\n",
    "\n",
    "# Select and show\n",
    "df.select(\"name\",\"age\").show()\n",
    "\n",
    "# Filter and show\n",
    "df.filter(df[\"age\"] > 30).show()\n",
    "\n",
    "# Where and show\n",
    "df.where(df[\"age\"] == 30).show()\n",
    "\n",
    "# Count and print using F Sting\n",
    "row_count = DataFrame_Name.count()\n",
    "print (f'Number of rows: {row_count}')\n",
    "\n",
    "# GroupBy on one column then show\n",
    "DataFrame_Name.groupBy('Column_Name1').Agg_Type('Column_Names').show()\n",
    "\n",
    "# GroupBy on more than one column by using \"agg\" then show\n",
    "DataFrame_Name.groupBy('Column_Name1').agg('{'Column_Names':'Agg_Type'}').show()\n",
    "\n",
    "# Average \"salary\" for both \"entry level\" and \"Canada\" (Two filters and one groupBy) then show\n",
    "CA_jobs = ca_salaries_df.filter(ca_salaries_df['company_location'] == \"CA\").filter(ca_salaries_df['experience_level']== \"EN\").groupBy().avg(\"salary_in_usd\")\n",
    "CA_jobs.show()\n",
    "\n",
    "# Sort then show\n",
    "df.sort(\"age\", ascending= False).show()\n",
    "\n",
    "# Filter rows with column \"Salary\" above 500\n",
    "df.filter(df['salary'] > 500)\n",
    "\n",
    "# GroupBy column \"deparment\" and calculate avg of column \"salary\"\n",
    "df.groupBy(\"department\").avg(\"salary\")\n",
    "\n",
    "# Join\n",
    "df1.join(df2, df1.ID1 = df2.ID2, \"inner\")\n",
    "\n",
    "# Filter and groupBy to find the maximum salaries for large companies\n",
    "salaries_df.filter(salaries_df.company_size == \"L\").groupBy().max(\"salary_in_usd\").show()\n",
    "\n",
    "# Filter and groupBy to Average salaries at large us companies\n",
    "large_companies=salaries_df.filter(salaries_df.company_size == \"L\").filter(salaries_df.company_location == \"US\").groupBy().avg(\"salary_in_usd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data Manipulation with PySpark DataFrames\n",
    "\n",
    ".na.drop()                                              => to remove rows with null values\n",
    ".drop(\"column_name1\",\"column_name2\",...)                => remove unneccessary columns\n",
    ".isNotNull()                                            => to filter out nulls\n",
    ".na.fill({\"column_name\":value})                         => to replace nulls in a column with a specific value\n",
    ".withColumn(\"new_column_name\",expression)               => to add a new column with expression (formula based on other columns)\n",
    ".withColumnRenamed(\"new_column_name\",\"old_column_name\") => to rename an existing column\n",
    "\n",
    "\"\"\"\n",
    "# Drop nulls (missing values)\n",
    "df.na.drop()\n",
    "\n",
    "# Remove column named \"department\"\n",
    "df.drop(\"department\")\n",
    "\n",
    "# Filter out nulls in a column\n",
    "df.where(col(\"Column_Name\").IsNotNull())\n",
    "\n",
    "# Fill nulls in column \"age\" with 0\n",
    "df.fill({\"age\" : 0})\n",
    "\n",
    "# Create a new column named \"age_plus_five\"\n",
    "Newdf = df.withColumn(\"age_plus_five\", df['age']+5)\n",
    "\n",
    "# Rename the column \"age\" to \"years\"\n",
    "df.withColumnRenamed(\"age\",\"years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DataTypes in PySpark DataFrames:\n",
    "\n",
    "IntegerType()                       => A Data type that represents a Whole Number\n",
    "LongType()                          => A Data type that represents a Large whole number\n",
    "FloatType Or DoubleType             => A Data type that represents a Decimal values\n",
    "StringType()                        => A Data type that represents a Text or String\n",
    "ArrayType(StringType(),False)       => A Data type that represents a Array or list\n",
    "StrucField()                        => A Data type that represents a Field (column) and it's nullablity\n",
    "StrucType(StructField, DataType())  => A Data type that represents a group of related fields built by StrucFields\n",
    "MapType(StringType(),StringType())  => A Data type that represents a key-value pairs like dictionary\n",
    "\n",
    "Function                               Description\n",
    "---------                              ---------\n",
    "Array()                             => To build an array\n",
    "lit()                               => To add a new column by assigning a constant value to DataFrame\n",
    "\"\"\"\n",
    "\n",
    "# Using StrucType() and StrucField() to define a schema:\n",
    "from pyspark.sql.types import (StructType,StructField, IntegerType, StringType, ArrayType)\n",
    "\n",
    "# Construct the schema\n",
    "Defiend_schema = StructType([\n",
    "                        StructField(\"id\",IntegerType(),True),\n",
    "                        StructField(\"name\",StringType(),True),\n",
    "                        StructField(\"scores\",ArrayType(IntegerType()),True)\n",
    "                            ])\n",
    "# Set the schema on a DataFrame1 \n",
    "df1 = spark.createDataFrame (Your_Data, schema= Defiend_schema)\n",
    "# Set the schema on a DataFrame2 \n",
    "df2 = spark.read.csv (\"CSV_Name\", sep='Seperator', header = True , schema= Defiend_schema)\n",
    "# new way to print the schema\n",
    "df1.printSchema()\n",
    "\n",
    "# add an array column with constant value of 25\n",
    "from pyspark.sql.types import (Array, lit, StructField, StringType, MapType)\n",
    "df.withColumn(\"Column_Name\",Array(lit(25)))\n",
    "\n",
    "# define a Map (dictionary)\n",
    "StructField(\"ColumnName\",MapType(StringType(),StringType(), True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d11673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"User Defined Functions (udf) in PySpark (like the ones in pandas)\n",
    "\n",
    "***PySpark udf need spark session registration.\n",
    "***PySpark udf does columnar level changes\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define the function\n",
    "def to_uppercase(s):\n",
    "    return s.upper() if s else None\n",
    "\n",
    "# Register the function in spark session using PySpark UDF function and define it's data type\n",
    "to_uppercase_udf = udf(to_uppercase, StringType())\n",
    "\n",
    "# Appy the new function into column \"name\" and add new column named \"uppered\" \n",
    "df_New = df.withColumn(\"uppered\", to_uppercase_udf(df[\"name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bacab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Using Pandas_UDF function in PySpark\n",
    "\n",
    "***pandas_udf does not need spark session registration.\n",
    "***pandas_udf can be called outside the spark session.\n",
    "***pandas_udf does row level changes (Despite PySpark udf does columnar level changes).\n",
    "***pandas_udf outperform PySpark udf over performance for using large datasets cause runs outside the Spark session.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Define a Pandas UDF that adds 10 to each element in a vectorized way\n",
    "@pandas_udf(DoubleType())\n",
    "def add_ten_pandas(column):\n",
    "    return column + 10\n",
    "\n",
    "# Apply the UDF and show the result\n",
    "df.withColumn(\"10_plus\", add_ten_pandas(df['value']))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b768f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Resilient distributed datasets (RDD) in PySpark\n",
    "\n",
    "***RDD is the core of spark.\n",
    "***RDD does Paralellization and split data and computation into multiple worker nodes in a same cluster.\n",
    "***RDDs are immutable and connot be changed once created.\n",
    "***Workers node process data in parallel and combine at the end.\n",
    "***Faster process in working with large datasets (several terabytes).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe6986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating an RDD\n",
    "\n",
    "RDD function:\n",
    "paralelize()         => To create RDD \n",
    "collect()            => To collect and show a summary of the RDD's result\n",
    "Map()                => To apply a function into RDD\n",
    "\n",
    "RDD Methods:\n",
    ".rdd                 => To convert a DataFrame into RDD\n",
    ".toDF                => To convert back a RDD to a DataFrame\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "# Creat new spark session RDD1\n",
    "spark = SparkSession.builder.appName(\"RDD1\").getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = spark.createDataFrame([\n",
    "        Row(a=1, b=2., c='string1'),\n",
    "        Row(a=2, b=3., c='string2'),\n",
    "        Row(a=4, b=5., c='string3')\n",
    "                            ])\n",
    "# Convert the DataFrame into RDD\n",
    "Rdd1 = df.rdd\n",
    "\n",
    "# Show a summary of RDD\n",
    "Result1 = Rdd1.collect()\n",
    "\n",
    "# Print all RDD's result\n",
    "for row in Result1:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RDD V.S. DataFrames:\n",
    "\n",
    "RDDs offer a low-level interface, \n",
    "providing maximum flexibility. \n",
    "You can manipulate data at a granular level, \n",
    "but this flexibility comes at the cost of requiring more lines of code for even moderately complex operations. \n",
    "One strength of RDDs is their ability to preserve data types across operations. \n",
    "However, they lack the schema-aware optimizations of DataFrames, which means operations on structured data are less efficient and harder to express.\n",
    "While RDDs can scale to handle large datasets, they’re not as optimized for analytics as DataFrames. \n",
    "DataFrames are optimized for ease of use, providing a high-level abstraction for working with data. \n",
    "DataFrames encapsulate complex computations, making it easier to achieve our objectives with less code and fewer errors. \n",
    "One of the standout features of DataFrames is their SQL-like functionality. \n",
    "With SQL syntax, even complex transformations and analyses can be performed in just a few lines of code. \n",
    "DataFrames come with built-in schema awareness, meaning they contain column names and data types, just like a structured table in SQL. \n",
    "\n",
    "\n",
    "    DataFrame                                    RDD\n",
    "    -----                                        -----\n",
    "    Ease of use, SQL-like Operations             More code, More Error\n",
    "    built-in Schema (columns names and types)    No Schema (Hard to work with SQL)\n",
    "    Small Scale                                  Large Scale\n",
    "    good at Analytics                            Poor at Analytics\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a12a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Introduction to Spark SQL\n",
    "\n",
    "***Spark SQL is a module of Apache Spark for structured data processing.\n",
    "***Allows to run SQL Queries inside data processing\n",
    "***Combination of PySpark and SQL for complex workflows\n",
    "***Easy access to structured data (like SQL Sever)\n",
    "***Flexibility and speed for working with large datasets. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ef11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Spark SQL (Similar to Dynamic SQL Query)\n",
    "\n",
    "Formula ===> spark.sql(\"Query\")\n",
    "\"\"\"\n",
    "\n",
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL Exampple\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"Jack\", \"HR\", 30), (\"Bob\", \"IT\", 40), (\"Chat\",\"Sale\",25)]\n",
    "columns = [\"Name\",\"Department\",\"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Register DataFrame as temporary view\n",
    "df.createOrReplaceTempView(\"People\")\n",
    "\n",
    "# Runing query using spark sql by passing the query as string\n",
    "result = spark.sql (\"SELECT Name, Age FROM People WHERE Age > 30\") \n",
    "\n",
    "# Print the top 10\n",
    "result.show(10)\n",
    "\n",
    "# Print basic statistics\n",
    "result.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343c8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PySpark Aggregation\\n\\nCommonly used functions include:\\nSUM(), COUNT(), AVERAGE(), MAX(), and MIN() \\nThese are applied using either SQL queries with spark.sql()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"PySpark SQL functions for aggregation\n",
    "\n",
    "Commonly used functions include:\n",
    "SUM(), COUNT(), AVERAGE(), MAX(), and MIN() \n",
    "These are applied using either SQL queries with spark.sql()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf9a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Combining SQL operations and DataFrame\n",
    "\n",
    "***By combining these approaches, we get the best of both worlds: \n",
    "   the expressiveness of SQL and the programmatic control of the DataFrame\n",
    "\"\"\"\n",
    "\n",
    "# Example: Filteing data using DataFrame and then using SQL Operations:\n",
    "\n",
    "filtered_df = df.filter(df.salary > 3000)\n",
    "\n",
    "# Register filterd DataFarme as a View\n",
    "filtered_df.createOrReplaceTempView(\"filtered_employees\")\n",
    "\n",
    "# Aggregate using SQl on the view\n",
    "spark.sql (\"\"\"\n",
    "    SELECT Department, Count(*) As Employee_Count\n",
    "    FROM filtered_employees\n",
    "    GROUP BY Department\n",
    "           \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Handling data types in aggregations\n",
    "\n",
    "*** Data type mismatches can lead to errors or unexpected results\n",
    "*** PySpark provides functions and methods like `cast()` to convert data types before processing\n",
    "\"\"\"\n",
    "\n",
    "# Example:\n",
    "\n",
    "# Creating a DataFrame\n",
    "data = [(\"HR\",\"3000\"),(\"IT\",\"4000\"),(\"Finance\",\"3500\")]\n",
    "columns = [\"Department\",\"Salary\"]\n",
    "df = spark.createDataFrame (data, schema=columns)\n",
    "\n",
    "# Add a new column and casting data type\n",
    "df = df.withColumn(\"Integer_Salary\", df['Salary'].cast(\"int\"))\n",
    "\n",
    "# Perform aggregation\n",
    "df.groupBy(\"Department\").sum(\"Salary\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad00ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"RDD Aggregation ()\n",
    "*** The best way is using lambda funciton\n",
    "\"\"\"\n",
    "# RDD Agrregation for does the same as previous example\n",
    "\n",
    "# Map the DataFrame to an RDD\n",
    "rdd = df.rdd.map(lambda row: (row[\"Department\"], row[\"Salary\"]))\n",
    "\n",
    "# Apply a lambda function to get the sum of the DataFrame\n",
    "rdd_aggregated = rdd.reduceByKey(lambda x,y: x + y)\n",
    "\n",
    "# Show the collected Results\n",
    "print(rdd_aggregated.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Best practices for PySpark aggregations\n",
    "\n",
    "Here are some best practices for PySpark aggregations: \n",
    "***Filter early to reduce data size before performing aggregations. \n",
    "***Ensure data is clean and correctly typed. \n",
    "***Avoid using the entire dataset by minimizing operations like `.groupBy()`.\n",
    "***Choose the right API by prefering DataFrames for most tasks due to their optimizations.\n",
    "***Monitor performance, by using `explain()` to inspect the execution plan and optimize accordingly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b199009",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Optimizing PySpark jobs Method1: Scale\n",
    "\n",
    "***optimizing PySpark jobs becomes essential for managing performance, resource usage, and execution speed.\n",
    "***Methods like `broadcast()` will load a smaller dataset across the cluster, using all available compute.\n",
    "***broadcast() method is used to distribute a small dataset across all worker nodes, minimizing shuffling during join operations.\n",
    "\n",
    "\"\"\"\n",
    "# broadcast Method:\n",
    "joined_df = large_df.join(broadcast(small_df)), on=\"key_column\" , how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718198b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Optimizing PySpark jobs Method2: Execution plans\n",
    "\n",
    "***First, we’ll explore how to interpret Spark execution plans to identify performance bottlenecks\n",
    "***We can inspect and understand Spark execution plans process on its distributed cluster using the `.explain()` method. \n",
    "***The `explain()` method details the logical and physical plans for optimization.\n",
    "***By analyzing Spark execution plans, we can spot inefficiencies, like redundant shuffles or unoptimized joins, and address them before running the job.\n",
    "\n",
    "\"\"\"\n",
    "# explain Method to view execution plan:\n",
    "df.filter(df.Age > 40).select(\"Name\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed11a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Optimizing PySpark jobs Method3: Catching or Persisting\n",
    "\n",
    "***Second,we’ll discuss caching and persisting DataFrames, which can significantly speed up iterative queries.\n",
    "***When working with large datasets, we often reuse intermediate results across multiple operations. \n",
    "***Recomputing these results can be costly, especially when reading from disk. \n",
    "***To avoid this, we have two tools that keeps data readily available, caching and persisting. \n",
    "***The `.cache()` method stores a DataFrame in memory for fast access.\n",
    "***The `.persist()` method offers more flexibility by letting us choose storage levels.\n",
    "\n",
    "Catching    => Store data in memory for faster access for small datasets\n",
    "Persisting  => Store data into different storage levels (like hard disk) for larger datasets\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Catche the DataFrames into memory:\n",
    "df.cache()\n",
    "# Then perform desired operations on cached DataFrame like:\n",
    "df.filter(df[\"column1\"] > 50).show()\n",
    "# unpersist it after your use\n",
    "df.unpersist()\n",
    "\n",
    "\n",
    "# Persisting DataFrames into storagelevel\n",
    "from pyspark import StorageLevel\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "# Then Perform transformations like:\n",
    "result = df.groupBy(\"column2\".agg({\"column3\":\"sum\"}))\n",
    "# unpersist it after your use\n",
    "df.unpersist()\n",
    "\n",
    "\"\"\"\n",
    "***Persist method is especially useful for long-running jobs on large clusters. \"\n",
    "***However, caching consumes memory, so it’s important to uncache data when it’s no longer needed, using `.unpersist()` method.\"\n",
    "***In this example, if the DataFrame doesn’t fit in memory, Spark writes the overflow to disk. \n",
    "***This ensures our operations can still proceed without crashing due to resource constraints. \n",
    "***You will encounter it as you work with PySpark in the real world.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Optimizing PySpark jobs Method4: Using best practices\n",
    "\n",
    "here are a few best practices to optimize PySpark jobs: \n",
    "\n",
    "1) Use Small Subsections: \n",
    "Pick functions and methods like map() over whole dataset tools like groupBy() that require shuffles. \n",
    "\n",
    "2) Broadcast Joins: \n",
    "For small datasets, use broadcast() to load the dataset onto all nodes and avoid shuffles. \n",
    "\n",
    "3) Avoid Repeated Actions: Operations like count() or show() trigger jobs. Store intermediate results to prevent recomputation.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[Department#203], functions=[sum(Salary#205L)])\n",
      "   +- Exchange hashpartitioning(Department#203, 200), ENSURE_REQUIREMENTS, [plan_id=92]\n",
      "      +- HashAggregate(keys=[Department#203], functions=[partial_sum(Salary#205L)])\n",
      "         +- InMemoryTableScan [Department#203, Salary#205L]\n",
      "               +- InMemoryRelation [Name#202, Department#203, Age#204L, Salary#205L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(1) Scan ExistingRDD[Name#202,Department#203,Age#204L,Salary#205L]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Department: string, Age: bigint, Salary: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Optimizing PySpark Example\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"Jack\", \"HR\", 30, 1000), (\"Bob\", \"IT\", 40, 1500), (\"Chat\",\"Sale\",25, 2000)]\n",
    "columns = [\"Name\",\"Department\",\"Age\",\"Salary\"]\n",
    "\n",
    "# Load dataset into a DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Cache the DataFrame\n",
    "df.cache()\n",
    "\n",
    "# Perform aggregation\n",
    "agg_result = df.groupBy(\"Department\").sum(\"Salary\")\n",
    "\n",
    "# Analyze the execution plan\n",
    "agg_result.explain()\n",
    "\n",
    "# Uncache the DataFrame\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3aba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Advanced topics you can explore as you continue your PySpark journey (Not coverd yet):\n",
    "\n",
    "***advanced cluster configuration (cluster management)\n",
    "***performance optimization (spark job optimization)\n",
    "***big data applications (big data fundamentals with PySpark)\n",
    "***streaming data processing with PySpark (Clean data with PySpark)\n",
    "***Sparks machine learning pipelines and integration with cloud-based tools. Whether you’re a data engineer, a data scientist, or a machine learning engineer, you now have the skills to leverage PySpark for managing and analyzing big data.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
